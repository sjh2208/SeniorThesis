{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import TrainingArguments, Trainer, GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large', output_hidden_states=True, use_cache=False)\n",
    "model.train()\n",
    "\n",
    "print('Model successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized datset from disk...\n",
      "Dataset successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# Prepare/load finetuning dataset\n",
    "\n",
    "try:\n",
    "    # See if tokenized dataset already exists\n",
    "    tokenized_dataset = load_from_disk('./tokenized_dataset')\n",
    "    print('Loading tokenized datset from disk...')\n",
    "\n",
    "except:\n",
    "    # Download dataset\n",
    "    print('Downloading dataset...')\n",
    "    dataset = load_dataset('csv', data_files='./lm_data.csv', split='train')\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    print('Tokenizing dataset...')\n",
    "    max_length = 512\n",
    "    tokenized_dataset=dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=max_length, padding='max_length'), batched=True)\n",
    "    tokenized_dataset.save_to_disk('./tokenized_dataset')\n",
    "    \n",
    "# Store tokenized dataset\n",
    "train_dataset = tokenized_dataset\n",
    "\n",
    "print('Dataset successfully loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator and training arguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer,\n",
    "                                                mlm = False)\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./model_checkpoints',\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  save_total_limit = 2,\n",
    "                                  num_train_epochs=1,\n",
    "                                  gradient_checkpointing=True)\n",
    "\n",
    "print('Training parameters set!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "\n",
    "# Define the trainer class\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  data_collator=data_collator)\n",
    "\n",
    "# Run training and save the model\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir='./finetuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure new model loads in correctly\n",
    "\n",
    "new_model = GPT2LMHeadModel.from_pretrained('./finetuned_model', output_hidden_states=True)\n",
    "print(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
