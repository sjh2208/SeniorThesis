{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55697e2b-186c-463c-a6c9-790de25b558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import special, stats\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "\n",
    "# Load pre-trained model tokenizers (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Load modelss (weights)\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained('gpt2-large', use_cache=False)\n",
    "pretrained_model.eval()\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained('./finetuned_model', use_cache=False)\n",
    "finetuned_model.eval()\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "sentiment_model.eval()\n",
    "\n",
    "# Load and augment dataframe\n",
    "train_df = pd.read_csv('./train_data.csv')\n",
    "train_df['pre_ces'] = np.zeros(len(train_df.index))\n",
    "train_df['pre_ces'] = train_df['pre_ces'].astype('object')\n",
    "train_df['ft_ces'] = np.zeros(len(train_df.index))\n",
    "train_df['ft_ces'] = train_df['pre_ces'].astype('object')\n",
    "train_df['sentiment_scores'] = np.zeros(len(train_df.index))\n",
    "train_df['sentiment_scores'] = train_df['sentiment_scores'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c070259-183c-4671-af56-18473e3f581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for extracting cross-entropy and sentiment from the models\n",
    "\n",
    "def get_tokenized(prompt):\n",
    "    tokens=tokenizer.encode(prompt)\n",
    "    tokenized = [tokenizer.decode(t) for t in tokens]\n",
    "    return tokenized\n",
    "\n",
    "def get_token_nums(tokens):\n",
    "    nums = [tokenizer.encode(t) for t in tokens]\n",
    "    return nums\n",
    "\n",
    "def get_probs(tokens, model):\n",
    "    \n",
    "    #convert to tensor variable\n",
    "    tokens_tensor = torch.tensor([tokens])\n",
    "    \n",
    "    #get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "    \n",
    "    #compile probability distribution outputs\n",
    "    probs_list = [torch.softmax(predictions[0][i],-1).data.numpy() for i in range(len(predictions[0]))]\n",
    "        \n",
    "    return probs_list\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    return -np.sum(special.xlogy(p, q))\n",
    "\n",
    "def get_cross_entropy(tokens, probs):\n",
    "    \n",
    "    #q is the predicted distribution, p is the one-hot vector representing the actual next token\n",
    "    #q = probs[i][0], p = np.zeros(len(q)), p[tokens[i + 1]] = 1\n",
    "    \n",
    "    def hot(a, i):\n",
    "        a[tokens[i + 1]] = 1\n",
    "        return a\n",
    "\n",
    "    ces = [cross_entropy(hot(np.zeros(len(probs[i][0])), i), probs[i][0]) for i in range(len(probs) - 1)]\n",
    "    return ces\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    \n",
    "    tokens = sentiment_tokenizer.encode(text)[:512]\n",
    "    tokens_tensor = torch.tensor([tokens])\n",
    "    output = special.softmax(sentiment_model(tokens_tensor)[0][0].detach().numpy())\n",
    "    \n",
    "    return output\n",
    "\n",
    "def label(scores):\n",
    "    idx = np.argmax(scores)\n",
    "    if idx == 0:\n",
    "        return 'Negative'\n",
    "    elif idx == 1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849c323-03fd-4815-94ff-320aa7f3884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each training example\n",
    "\n",
    "cont = True\n",
    "\n",
    "if cont:\n",
    "    train_df = pd.read_csv('./results.csv')\n",
    "    for idx in range(len(train_df.index)):\n",
    "        if pd.isna(train_df.loc[idx, 'sentiment_labels']):\n",
    "            print('Starting from:', idx)\n",
    "            start = idx\n",
    "            break\n",
    "        else:\n",
    "            start = 0\n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "print('Starting processing...')\n",
    "for i in range(start, len(train_df.index), 1):\n",
    "    \n",
    "    print(i, '/', len(train_df.index))\n",
    "    \n",
    "    prompt = train_df.loc[i, 'text']\n",
    "    \n",
    "    tokenized = get_tokenized(prompt)[:1024]\n",
    "    \n",
    "    tokens = get_token_nums(tokenized)\n",
    "    pretrained_probs = get_probs(tokens, pretrained_model)\n",
    "    finetuned_probs = get_probs(tokens, finetuned_model)\n",
    "    \n",
    "    train_df.at[i, 'pre_ces'] = get_cross_entropy(tokens, pretrained_probs)\n",
    "    train_df.at[i, 'ft_ces'] = get_cross_entropy(tokens, finetuned_probs)\n",
    "    train_df.at[i, 'sentiment_scores'] = get_sentiment_scores(prompt)\n",
    "    train_df.at[i, 'sentiment_labels'] = label(train_df.at[i, 'sentiment_scores'])\n",
    "    \n",
    "    train_df.to_csv('./results.csv', index=False)\n",
    "    \n",
    "print('Finished processing!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee0415-7b83-464f-980d-6dbe6616ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training examples in batches\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for i in range(0, len(train_df.index), batch_size):\n",
    "    \n",
    "    batch_df = pd.DataFrame(train_df[i:i+batch_size]['text']).reset_index()\n",
    "\n",
    "    # Tokenize dataset\n",
    "    print('Tokenizing dataset...')\n",
    "    dataset = Dataset.from_pandas(batch_df)\n",
    "    max_length = 1024\n",
    "    tokenized_dataset=dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=max_length, padding='max_length'), batched=True)\n",
    "    train_dataset = tokenized_dataset\n",
    "\n",
    "    print('Dataset successfully loaded!')\n",
    "    \n",
    "    # Define data collator and training arguments\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer,\n",
    "                                                mlm = False)\n",
    "\n",
    "    training_args = TrainingArguments(output_dir='.nothing',\n",
    "                                      optim='adafactor',\n",
    "                                      per_device_eval_batch_size=1,\n",
    "                                      num_train_epochs=1,\n",
    "                                      gradient_checkpointing=True)\n",
    "\n",
    "    print('Training parameters set!')\n",
    "    \n",
    "    # Evaluate examples\n",
    "    pretrained_trainer = Trainer(model=pretrained_model,\n",
    "                                 args=training_args,\n",
    "                                 data_collator=data_collator)\n",
    "\n",
    "    pre_results = pretrained_trainer.predict(train_dataset)\n",
    "    \n",
    "    finetuned_trainer = Trainer(model=finetuned_model,\n",
    "                                args=training_args,\n",
    "                                data_collator=data_collator)\n",
    "    \n",
    "    ft_results = finetuned_trainer.predict(train_dataset)\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        \n",
    "        idx = i + j\n",
    "        \n",
    "        text = batch_df.loc[j, 'text']\n",
    "        tokens = get_tokenized(text)[:1024]\n",
    "        token_nums = get_token_nums(tokens)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        pre_tens = torch.tensor([pre_results[0][j][0:num_tokens]])\n",
    "        ft_tens = torch.tensor([ft_results[0][0][j][0:num_tokens]])\n",
    "        \n",
    "        pre_output = torch.softmax(pre_tens, -1).data.numpy()[0].tolist()\n",
    "        ft_output = torch.softmax(ft_tens, -1).data.numpy()[0].tolist()\n",
    "        \n",
    "        train_df.at[idx, 'pre_ces'] = get_cross_entropy(token_nums, pre_output)\n",
    "        train_df.at[idx, 'ft_ces'] = get_cross_entropy(token_nums, ft_output)\n",
    "        \n",
    "        train_df.at[idx, 'sentiment_scores'] = get_sentiment_scores(text)\n",
    "        train_df.at[idx, 'sentiment_label'] = label(train_df.at[idx, 'sentiment_scores'])\n",
    "        \n",
    "        train_df.to_csv('./results.csv', index=False)\n",
    "        print('Done:', idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
