{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3771d23-d1b2-46f4-8a57-c0a29ab6f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "def p_ast(pvalue):\n",
    "    if pvalue <= 0.0001:\n",
    "        return \"****\"\n",
    "    elif pvalue <= 0.001:\n",
    "        return \"***\"\n",
    "    elif pvalue <= 0.01:\n",
    "        return \"**\"\n",
    "    elif pvalue <= 0.05:\n",
    "        return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def greater(lis, bench):\n",
    "    count = 0\n",
    "    for l in lis:\n",
    "        if l > bench:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def get_crit_diff(list1, list2):\n",
    "    df = len(list1) + len(list2) - 2\n",
    "    crit_t = stats.t.ppf(0.05, df)\n",
    "    denom = math.sqrt(((np.std(list1)**2)/len(list1)) + ((np.std(list2)**2)/len(list2)))\n",
    "    crit_diff = math.sqrt((crit_t * denom)**2)\n",
    "    return crit_diff\n",
    "\n",
    "def get_tokenized(prompt):\n",
    "    tokens=tokenizer.encode(prompt)\n",
    "    tokenized = [tokenizer.decode(t) for t in tokens]\n",
    "    return tokenized\n",
    "\n",
    "def get_token_nums(tokens):\n",
    "    nums = [tokenizer.encode(t) for t in tokens]\n",
    "    return nums\n",
    "    \n",
    "df = pd.read_csv('./results.csv').dropna()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204984bc-a280-4a31-83ec-2a56f425bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gaze durations by news type\n",
    "\n",
    "eye_df = pd.read_csv('./eyetracking.csv')\n",
    "violin_df = pd.DataFrame(columns=['dur', 'Veracity'])\n",
    "\n",
    "for i in range(len(eye_df.index)):\n",
    "    label = eye_df.loc[i, 'version']\n",
    "    dur = eye_df.loc[i, 'viewingTimeSec']\n",
    "    if label == 'true':\n",
    "        df1 = pd.DataFrame(data={'dur':[float(dur)], 'Veracity': [str('True Stories')]})\n",
    "        violin_df = pd.concat([violin_df, df1])\n",
    "    elif label == 'fake':\n",
    "        df1 = pd.DataFrame(data={'dur':[float(dur)], 'Veracity': [str('Fake Stories')]})\n",
    "        violin_df = pd.concat([violin_df, df1])\n",
    "true = violin_df[violin_df['Veracity'] == 'True Stories']['dur'].tolist()\n",
    "fake = violin_df[violin_df['Veracity'] == 'Fake Stories']['dur'].tolist()\n",
    "\n",
    "stat, pvalue = stats.ttest_ind(true, fake)\n",
    "\n",
    "display(violin_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.violinplot(data=violin_df, order=['True Stories', 'Fake Stories'], inner='quartile')\n",
    "ax.set_ylabel('Gaze Duration (sec)')\n",
    "ax.set_title('Gaze Duration by News Type')\n",
    "ax.text(x = 0.5, y = max([np.mean(true), np.mean(fake)]) + 30, s=p_ast(pvalue), fontsize=15, horizontalalignment='center')\n",
    "fig.savefig('./figures/fig1.png')\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame(data={'Veracity':['True', 'Fake'],\n",
    "                               'N':[len(true), len(fake)],\n",
    "                               'Avg. Gaze Duration':[np.mean(true), np.mean(fake)],\n",
    "                               'SD. Gaze Duration':[np.std(true), np.std(fake)],\n",
    "                               't-stat':[stat, stat],\n",
    "                               'p-value':[pvalue, pvalue]})\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbab2f-eeb5-4c24-b8ce-7f12c132b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average CE in stories\n",
    "\n",
    "pre_true = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['pre_ces']]]\n",
    "pre_fake = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['pre_ces']]]\n",
    "ft_true = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "ft_fake = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "pt_mean = np.mean(pre_true)\n",
    "pf_mean = np.mean(pre_fake)\n",
    "ft_mean = np.mean(ft_true)\n",
    "ff_mean = np.mean(ft_fake)\n",
    "pre_stat, pre_pvalue = stats.ttest_ind(pre_true, pre_fake)\n",
    "ft_stat, ft_pvalue = stats.ttest_ind(ft_true, ft_fake)\n",
    "\n",
    "violin_df = pd.DataFrame(columns = ['ce', 'Model', 'Veracity'])\n",
    "for i in pre_true:\n",
    "    df1 = {'ce':i, 'Model':'Pretrained', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in pre_fake:\n",
    "    df1 = {'ce':i, 'Model':'Pretrained', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_true:\n",
    "    df1 = {'ce':i, 'Model':'Finetuned', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_fake:\n",
    "    df1 = {'ce':i, 'Model':'Finetuned', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.violinplot(data=violin_df, x='Model', y='ce', hue='Veracity', inner='quartile')\n",
    "sns.move_legend(ax, 'upper left')\n",
    "ax.set_ylabel('Avg. Cross-Entropy in Stories (nats)')\n",
    "ax.set_title('Average Cross-Entropy by Model and News Type')\n",
    "ax.text(x = 0, y = max([pt_mean, pf_mean]) + 2, s=p_ast(pre_pvalue), fontsize=15, horizontalalignment='center')\n",
    "ax.text(x = 1, y = max([ft_mean, ff_mean]) + 2, s=p_ast(ft_pvalue), fontsize=15, horizontalalignment='center')\n",
    "fig.savefig('./figures/fig2.png')\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame(data={'Model':['Pretrained', 'Pretrained', 'Finetuned', 'Finetuned'],\n",
    "                               'Veracity':['True', 'Fake', 'True', 'Fake'],\n",
    "                               'N':[len(pre_true), len(pre_fake), len(ft_true), len(ft_fake)],\n",
    "                               'Avg. C-E':[pt_mean, pf_mean, ft_mean, ff_mean],\n",
    "                               'SD. C-E':[np.std(pre_true), np.std(pre_fake), np.std(ft_true), np.std(ft_fake)],\n",
    "                               'Within-model t-stat':[pre_stat, np.nan, ft_stat, np.nan],\n",
    "                               'Within-model p-value':[pre_pvalue, np.nan, ft_pvalue, np.nan]})\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd858c-c03d-447c-9eec-8f20320d516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average CE in stories\n",
    "\n",
    "pre_true = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['pre_ces']]]\n",
    "pre_fake = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['pre_ces']]]\n",
    "ft_true = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "ft_fake = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "pt_mean = np.mean(pre_true)\n",
    "pf_mean = np.mean(pre_fake)\n",
    "ft_mean = np.mean(ft_true)\n",
    "ff_mean = np.mean(ft_fake)\n",
    "pre_stat, pre_pvalue = stats.ttest_ind(pre_true, pre_fake)\n",
    "ft_stat, ft_pvalue = stats.ttest_ind(ft_true, ft_fake)\n",
    "\n",
    "violin_df = pd.DataFrame(columns = ['std', 'Model', 'Veracity'])\n",
    "for i in pre_true:\n",
    "    df1 = {'std':i, 'Model':'Pretrained', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in pre_fake:\n",
    "    df1 = {'std':i, 'Model':'Pretrained', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_true:\n",
    "    df1 = {'std':i, 'Model':'Finetuned', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_fake:\n",
    "    df1 = {'std':i, 'Model':'Finetuned', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.violinplot(data=violin_df, x='Model', y='std', hue='Veracity', inner='quartile')\n",
    "sns.move_legend(ax, 'upper right')\n",
    "ax.set_ylabel('Cross-Entropy StDev. in Stories (nats)')\n",
    "ax.set_title('Cross-Entropy StDev. by Model and News Type')\n",
    "ax.text(x = 0, y = max([pt_mean, pf_mean]) + 2, s=p_ast(pre_pvalue), fontsize=15, horizontalalignment='center')\n",
    "ax.text(x = 1, y = max([ft_mean, ff_mean]) + 2, s=p_ast(ft_pvalue), fontsize=15, horizontalalignment='center')\n",
    "fig.savefig('./figures/fig5.png')\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame(data={'Model':['Pretrained', 'Pretrained', 'Finetuned', 'Finetuned'],\n",
    "                               'Veracity':['True', 'Fake', 'True', 'Fake'],\n",
    "                               'N':[len(pre_true), len(pre_fake), len(ft_true), len(ft_fake)],\n",
    "                               'Avg. C-E':[pt_mean, pf_mean, ft_mean, ff_mean],\n",
    "                               'SD. C-E':[np.std(pre_true), np.std(pre_fake), np.std(ft_true), np.std(ft_fake)],\n",
    "                               'Within-model t-stat':[pre_stat, np.nan, ft_stat, np.nan],\n",
    "                               'Within-model p-value':[pre_pvalue, np.nan, ft_pvalue, np.nan]})\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e4473-4a84-4008-9c95-d5e94f7bf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare maximum cross-entropies in stories\n",
    "\n",
    "pre_true = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['pre_ces']]]\n",
    "pre_fake = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['pre_ces']]]\n",
    "ft_true = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "ft_fake = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "pt_mean = np.mean(pre_true)\n",
    "pf_mean = np.mean(pre_fake)\n",
    "ft_mean = np.mean(ft_true)\n",
    "ff_mean = np.mean(ft_fake)\n",
    "pre_stat, pre_pvalue = stats.ttest_ind(pre_true, pre_fake)\n",
    "ft_stat, ft_pvalue = stats.ttest_ind(ft_true, ft_fake)\n",
    "\n",
    "violin_df = pd.DataFrame(columns = ['ce', 'Model', 'Veracity'])\n",
    "for i in pre_true:\n",
    "    df1 = {'ce':i, 'Model':'Pretrained', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in pre_fake:\n",
    "    df1 = {'ce':i, 'Model':'Pretrained', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_true:\n",
    "    df1 = {'ce':i, 'Model':'Finetuned', 'Veracity':'True Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "for i in ft_fake:\n",
    "    df1 = {'ce':i, 'Model':'Finetuned', 'Veracity':'Fake Stories'}\n",
    "    violin_df = violin_df.append(df1, ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.violinplot(data=violin_df, x='Model', y='ce', hue='Veracity', inner='quartile')\n",
    "sns.move_legend(ax, 'upper right')\n",
    "ax.set_ylabel('Max. Cross-Entropy in Stories (nats)')\n",
    "ax.set_title('Maximum Cross-Entropy by Model and News Type')\n",
    "ax.text(x = 0, y = max([pt_mean, pf_mean]) + 10, s=p_ast(pre_pvalue), fontsize=15, horizontalalignment='center')\n",
    "ax.text(x = 1, y = max([ft_mean, ff_mean]) + 10, s=p_ast(ft_pvalue), fontsize=15, horizontalalignment='center')\n",
    "fig.savefig('./figures/fig3.png')\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame(data={'Model':['Pretrained', 'Pretrained', 'Finetuned', 'Finetuned'],\n",
    "                               'Veracity':['True', 'Fake', 'True', 'Fake'],\n",
    "                               'N':[len(pre_true), len(pre_fake), len(ft_true), len(ft_fake)],\n",
    "                               'Avg. Max C-E':[pt_mean, pf_mean, ft_mean, ff_mean],\n",
    "                               'SD. Max C-E':[np.std(pre_true), np.std(pre_fake), np.std(ft_true), np.std(ft_fake)],\n",
    "                               'Within-model t-stat':[pre_stat, np.nan, ft_stat, np.nan],\n",
    "                               'Within-model p-value':[pre_pvalue, np.nan, ft_pvalue, np.nan]})\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88c221-3784-47b4-b035-0ff057f5fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sentiment distributions\n",
    "\n",
    "true = df[df['label'] == 0]['sentiment_labels'].value_counts().sort_index()\n",
    "fake = df[df['label'] == 1]['sentiment_labels'].value_counts().sort_index()\n",
    "obsv = np.array([true, fake])\n",
    "\n",
    "chi_stat, chi_p, dof, exp = stats.chi2_contingency(obsv)\n",
    "\n",
    "x = np.arange(0, 2, 1)\n",
    "w = 0.3\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "negative = ax.bar(x - w, height=[true['Negative'], fake['Negative']], width=w, \n",
    "                  label='Negative', color='tab:gray', alpha=0.8)\n",
    "netrual = ax.bar(x, height=[true['Neutral'], fake['Neutral']], width=w, \n",
    "                  label='Neutral', color='tab:olive', alpha=0.8)\n",
    "positive = ax.bar(x + w, height=[true['Positive'], fake['Positive']], width=w, \n",
    "                  label='Positive', color='tab:cyan', alpha=0.8)\n",
    "ax.set_xticks(x, ['True Stories', 'Fake Stories'])\n",
    "ax.set_xlabel('News Story Type')\n",
    "ax.set_ylabel('Number of Examples w/ Sentiment Label')\n",
    "ax.set_title('Sentiment by News Type')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig('./figures/fig4.png')\n",
    "plt.show()\n",
    "\n",
    "result_df = pd.DataFrame(data={'Veracity':['True', 'Fake'],\n",
    "                               'N':[sum(true), sum(fake)],\n",
    "                               'Negative':[true['Negative'], fake['Negative']],\n",
    "                               'Neutral':[true['Neutral'], fake['Neutral']],\n",
    "                               'Positive':[true['Positive'], fake['Positive']],\n",
    "                               'Two-way chi-sq stat':[chi_stat, np.nan],\n",
    "                               'Two-way chi-sq p-value':[chi_p, np.nan]})\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7012dec-0d8c-459c-9cc8-d584de0873d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of continuous sentiment scores\n",
    "\n",
    "true = [list(map(float, j)) for j in [i[1:-1].split() for i in df[(df['label'] == 0)]['sentiment_scores']]]\n",
    "true_cont = [(1*i[0] + 2*i[1] + 3*i[2]) for i in true]\n",
    "\n",
    "fake = [list(map(float, j)) for j in [i[1:-1].split() for i in df[(df['label'] == 1)]['sentiment_scores']]]\n",
    "fake_cont = [(1*i[0] + 2*i[1] + 3*i[2]) for i in fake]\n",
    "\n",
    "d = {'True Stories':true_cont,\n",
    "     'Fake Stories':fake_cont}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.kdeplot(data=d, common_norm=True, fill=True)\n",
    "ax.set_xlabel('Continuous Sentiment Score')\n",
    "ax.set_title('Distribution of Sentiment Scores by Story Type')\n",
    "\n",
    "fake_x = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[0]\n",
    "fake_y = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[1]\n",
    "true_x = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[0]\n",
    "true_y = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[1]\n",
    "\n",
    "pd.DataFrame(data={'fake_sent':fake_x, 'fake_prob':fake_y, 'true_sent':true_x, 'true_prob':true_y}).to_csv('./analysis_results/sentiment_dist.csv', index=False)\n",
    "fig.savefig('./figures/fig4_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19658606-a908-4fd9-8b97-87198236d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of average cross-entropy\n",
    "\n",
    "true = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "fake = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "\n",
    "d = {'True Stories':true,\n",
    "     'Fake Stories':fake}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.kdeplot(data=d, common_norm=True, fill=True)\n",
    "ax.set_xlabel('Average Cross-Entropy in Stories (nats)')\n",
    "ax.set_title('Distribution of Average Cross-Entropy by Story Type')\n",
    "\n",
    "fake_x = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[0]\n",
    "fake_y = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[1]\n",
    "true_x = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[0]\n",
    "true_y = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[1]\n",
    "\n",
    "pd.DataFrame(data={'fake_ce':fake_x, 'fake_prob':fake_y, 'true_ce':true_x, 'true_prob':true_y}).to_csv('./analysis_results/avg_ce_dist.csv', index=False)\n",
    "fig.savefig('./figures/fig2_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f836ab-db4f-4aeb-ad23-302dc49f31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of std in cross-entropy\n",
    "\n",
    "true = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "fake = [np.std(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "\n",
    "d = {'True Stories':true,\n",
    "     'Fake Stories':fake}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.kdeplot(data=d, common_norm=True, fill=True)\n",
    "ax.set_xlabel('Average Cross-Entropy in Stories (nats)')\n",
    "ax.set_title('Distribution of SD in Cross-Entropy by Story Type')\n",
    "\n",
    "fake_x = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[0]\n",
    "fake_y = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[1]\n",
    "true_x = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[0]\n",
    "true_y = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[1]\n",
    "\n",
    "pd.DataFrame(data={'fake_ce':fake_x, 'fake_prob':fake_y, 'true_ce':true_x, 'true_prob':true_y}).to_csv('./analysis_results/std_ce_dist.csv', index=False)\n",
    "fig.savefig('./figures/fig5_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6e5bf-c324-4487-9f21-403e3522f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of maximum cross entropy\n",
    "\n",
    "true = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "fake = [max(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "\n",
    "d = {'True Stories':true,\n",
    "     'Fake Stories':fake}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax = sns.kdeplot(data=d, common_norm=True, fill=True)\n",
    "ax.set_xlabel('Max. Cross-Entropy in Stories (nats)')\n",
    "ax.set_title('Distribution of Maximum Cross-Entropy by Story Type')\n",
    "\n",
    "fake_x = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[0]\n",
    "fake_y = sns.kdeplot(data=d, common_norm=True).lines[0].get_data()[1]\n",
    "true_x = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[0]\n",
    "true_y = sns.kdeplot(data=d, common_norm=True).lines[1].get_data()[1]\n",
    "\n",
    "pd.DataFrame(data={'fake_ce':fake_x, 'fake_prob':fake_y, 'true_ce':true_x, 'true_prob':true_y}).to_csv('./analysis_results/max_ce_dist.csv', index=False)\n",
    "fig.savefig('./figures/fig3_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e932aa6-5d98-4223-a601-b65d4f2dc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(columns = ['text', 'label', 'pre_max_words', 'ft_max_words'])\n",
    "\n",
    "for i in range(100):\n",
    "    label = df.at[i, 'label']\n",
    "    pre_ces = list(map(float, df.at[i, 'pre_ces'][1:-1].split(', '))) \n",
    "    ft_ces = list(map(float, df.at[i, 'ft_ces'][1:-1].split(', ')))\n",
    "    tokenized = get_tokenized(df.at[i, 'text'])[:1024]\n",
    "    tokens = get_token_nums(tokenized)\n",
    "    \n",
    "    pre_max_idx = np.argmax(pre_ces) + 1\n",
    "    ft_max_idx = np.argmax(ft_ces) + 1\n",
    "    \n",
    "    pre_max_tokens = tokenized[pre_max_idx - 2:(pre_max_idx + 5)]\n",
    "    ft_max_tokens = tokenized[ft_max_idx - 2:(ft_max_idx + 5)]\n",
    "    \n",
    "    pre_max_words = ''.join(pre_max_tokens)\n",
    "    ft_max_words = ''.join(ft_max_tokens)\n",
    "    \n",
    "    df1 = {'text':df.at[i, 'text'], 'label':label, 'pre_max_words':pre_max_words, 'ft_max_words':ft_max_words}\n",
    "    words_df = words_df.append(df1, ignore_index=True)\n",
    "    \n",
    "display(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b82ea-ceae-4f68-8e61-3c6f8c02f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sent = [list(map(float, j)) for j in [i[1:-1].split() for i in df[(df['label'] == 0)]['sentiment_scores']]]\n",
    "true_contsent = [(1*i[0] + 2*i[1] + 3*i[2]) for i in true_sent]\n",
    "\n",
    "fake_sent = [list(map(float, j)) for j in [i[1:-1].split() for i in df[(df['label'] == 1)]['sentiment_scores']]]\n",
    "fake_contsent = [(1*i[0] + 2*i[1] + 3*i[2]) for i in fake_sent]\n",
    "\n",
    "true_ce = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 0]['ft_ces']]]\n",
    "fake_ce = [np.mean(list(map(float, j))) for j in [i[1:-1].split(', ') for i in df[df['label'] == 1]['ft_ces']]]\n",
    "\n",
    "xmin = 1.0\n",
    "xmax = 3.0\n",
    "ymin = 5.5\n",
    "ymax = 8\n",
    "xx, yy = np.mgrid[xmin:xmax:500j, ymin:ymax:500j]\n",
    "\n",
    "xt = true_contsent\n",
    "xf = fake_contsent\n",
    "yt = true_ce\n",
    "yf = fake_ce\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "values_t = np.vstack([xt, yt])\n",
    "values_f = np.vstack([xf, yf])\n",
    "kernel_t = stats.gaussian_kde(values_t)\n",
    "kernel_f = stats.gaussian_kde(values_f)\n",
    "f1 = np.reshape(kernel_t(positions).T, xx.shape)\n",
    "f2 = np.reshape(kernel_f(positions).T, xx.shape)\n",
    "\n",
    "res_df = pd.DataFrame(columns=['s', 'ce', 'pt', 'pf'])\n",
    "for i in range(len(xx)):\n",
    "    for j in range(len(yy)):\n",
    "        res_df.loc[(i*len(yy)) + j, 's'] = xx[i][j]\n",
    "        res_df.loc[(i*len(yy)) + j, 'ce'] = yy[i][j]\n",
    "        res_df.loc[(i*len(yy)) + j, 'pt'] = f1[i][j]\n",
    "        res_df.loc[(i*len(yy)) + j, 'pf'] = f2[i][j]\n",
    "res_df.to_csv('./analysis_results/3d.csv', index=False)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "cfset1 = ax.contourf(xx, yy, f1, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f1), cmap='coolwarm')\n",
    "cset1 = ax.contour(xx, yy, f1, colors='k')\n",
    "ax.clabel(cset1, inline=1, fontsize=10)\n",
    "\n",
    "cfset2 = ax.contourf(xx, yy, f2, cmap='coolwarm')\n",
    "ax.imshow(np.rot90(f2), cmap='coolwarm')\n",
    "cset2 = ax.contour(xx, yy, f2, colors='k')\n",
    "ax.clabel(cset2, inline=1, fontsize=10)\n",
    "\n",
    "fig.set_size_inches(7.5, 7.5)\n",
    "\n",
    "ax.set_xlabel('Continuous Sentiment Score')\n",
    "ax.set_ylabel('Average Cross-Entropy in Story (nats)')\n",
    "ax.set_title('2D Gaussian Distribution of Sentiment and Cross-Entropy by Story Type', y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d78e7c-9a63-400f-b5a8-c9ede245951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "surf1 = ax.plot_surface(xx, yy, f1, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "surf2 = ax.plot_surface(xx, yy, f2, rstride=1, cstride=1, cmap='plasma', edgecolor='none')\n",
    "\n",
    "ax.set_xlabel('Continuous Sentiment Score')\n",
    "ax.set_ylabel('Average Cross-Entropy (nats)')\n",
    "ax.set_zlabel('PDF')\n",
    "ax.set_title('2D Gaussian Distribution of Sentiment and Cross-Entropy by Story Type', y=1)\n",
    "fig.colorbar(surf1, shrink=0.5, location='bottom', orientation='horizontal', label='True news stories', pad=0.0005) # add color bar indicating the PDF\n",
    "fig.colorbar(surf2, shrink=0.5, location='bottom', orientation='horizontal', label='Fake news stories', pad=0.0005) # add color bar indicating the PDF\n",
    "ax.invert_xaxis()\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.view_init(20, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe8256-29d9-43a1-ba1e-c36f0a5e160d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
